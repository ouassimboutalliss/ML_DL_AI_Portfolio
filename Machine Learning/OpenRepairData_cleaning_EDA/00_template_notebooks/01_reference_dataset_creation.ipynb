{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference dataset creation\n",
    "The goal of this phase is to perform an initial exploration of the data and preprocess it in order to arrive at a reference dataset that can be used in subsequent (exploration and modelling) phases. To this end, the data that was delivered is converted into an appropriate format (e.g. CSV), described and explored via statistical and visual means, initially preprocessed so that missing values, outliers and noise are appropriately dealt with, enriched with other datasets (if relevant), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data characteristics\n",
    "> Describe some relevant summary statistics of the raw data\n",
    "> - total number of instances and attributes\n",
    "> - time period covered for time-sensitive attributes\n",
    "> - min/max/mean/std of most important attributes\n",
    "> - ...\n",
    ">\n",
    "> Typically, your output includes a table with a row for each attribute and a column for the min/max/mean/median. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting\n",
    "> Reformat the data so that particular data types are explicitly represented and explain what type of initial cleaning steps you need to perform, typically this involves\n",
    "> - type and date conversions\n",
    "> - removing strange text formats\n",
    "> - string to floating point conversion\n",
    "> - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "> Discuss some statistics on missing values and explain how you deal with them (e.g. imputing them, discarding them, replacing them with mean/previous/next value, leaving them in the data, ...) and explain why you took this approach. If you impute them using interpolation, kNN or some other advanced means, you should probably also explain that conceptually or refer to a web page where this is explained. \n",
    ">\n",
    "> Typically, you include a table which includes for each attribute how much percentage of the data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier identification and removal\n",
    "> Explain for what kind of outliers you check and how you check it. Typically you provide a plot on which the outliers are easy to spot, and you apply for example the 3-sigma rule to explicitly identify outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration\n",
    "> If there are several datasets, explain conceptually how you integrate them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection\n",
    "> Explain which instances and attributes from the original data you consider and which ones you discard (if any) and explain the motivation behind.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final dataset characteristics\n",
    "> Export the final dataset(s) to a CSV file (or another suitable format), so they are available for subsequent analyses, also by other people. Present the final characteristics, e.g. remaining number of instances, relevant attributes, coverage in time, min/max/mean/std values of most relevant attributes, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
